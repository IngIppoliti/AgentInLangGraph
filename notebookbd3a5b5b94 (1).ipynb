{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dotenv import load_dotenv\n_ = load_dotenv()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.tools.tavily_search import TavilySearchResults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TOOL THAT THE AGENT WILL HAVE TO HIS DISPOSAL\n\ntool = TavilySearchResults(max_results=4) #increased number of results\nprint(type(tool))\nprint(tool.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#create the agent state. An annotated list of messages that we want to add\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Agent:\n\n    def __init__(self, model, tools, system=\"\"):  #a model to use, tools to use and a system message.\n        self.system = system\n        graph = StateGraph(AgentState) #CREATE GRAPH passing the state.\n        graph.add_node(\"llm\", self.call_openai) #CREAT E NODE LLM ADD TO GRAPH and an action to represent this node.\n        graph.add_node(\"action\", self.take_action) #CREATE NODE ACTION ADD TO GRAPH and pass also the function to represent this node.\n        graph.add_conditional_edges( #CREATE NODE ACTION ADD TO GRAPH and pass also the function to represent this node.\n            \"llm\", #where di edge start\n            self.exists_action,  #function\n            {True: \"action\", False: END} #dictionary. How to map the response of the function. If the function response TRUE go to the action otherwise END\n        )\n        graph.add_edge(\"action\", \"llm\") #CREATE an EDGE from action to llm\n        graph.set_entry_point(\"llm\") #SET ENTRY POINT FOR THE GRAPH\n        self.graph = graph.compile() #COMPILE THE GRAPH\n        self.tools = {t.name: t for t in tools} #TOOLS DICTIONARY: mapping the name of the tool to the tool itself\n        self.model = model.bind_tools(tools) #BIND THE TOOLS, IS LETTING THE MODEL KNOW HAVING THOSE TOOLS THAT IT CAN CALL\n\n    def exists_action(self, state: AgentState):  #FUNCTION THAT REPRESENT THE EDGE DECISION NODE\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState): #FUNCTION THAT REPRESENT THE llm NODE. (AgentState is passed to all the node so all the function)\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages #CONCATENATE THE SYSTEM MESSAGE (SYSTEM PROMPT) TO THE OTHER MESSAGES (USER PROMPTS)\n        message = self.model.invoke(messages) #INVOKE THE MODEL \n        return {'messages': [message]} #RITORNA TUTTO IL PROMPT + ULTIMO MESSAGE (ANSWER DELL' LLM)\n\n    def take_action(self, state: AgentState): #FUNTION THAT REPRESENT THE ACTION NODE\n        tool_calls = state['messages'][-1].tool_calls #TAKE LAST MESSAGE TAKE THE ATTRIBUTE OF THE AIMessage (TYPE OF LANGCHAIN) THAT IS A LIST OF TOOLS\n        results = []\n        for t in tool_calls: #FOR EACH TOOL \n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # IF IT'S IN THE ALLOWED ONE (check for bad tool name from LLM)\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        return {'messages': results}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\nYou are allowed to make multiple calls (either together or in sequence). \\\nOnly look up information when you are sure of what you want. \\\nIf you need to look up some information before asking a follow up question, you are allowed to do that!\n\"\"\"\n\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")  #reduce inference cost\nabot = Agent(model, [tool], system=prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This allow you to print the draw of the graph.\n\nfrom IPython.display import Image\n\nImage(abot.graph.get_graph().draw_png())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [HumanMessage(content=\"What is the weather in sf?\")]\nresult = abot.graph.invoke({\"messages\": messages})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#LOG OF THE PREVIOUS GRAPH INVOCATION\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_PvPN1v7bHUxOdyn4J2xJhYOX'}\n#Back to the model!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result['messages'][-1].content","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [HumanMessage(content=\"What is the weather in SF and LA?\")]\nresult = abot.graph.invoke({\"messages\": messages})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#LOG OF THE PREVIOUS GRAPH INVOCATION\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_1SqGYuEtOOFN1yiIHSQTPnvE'}\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in Los Angeles'}, 'id': 'call_8RiM72Y7G8V7c3HEEAML1SKP'}\n#Back to the model!\n\n#AS you can see here before to go back to the model it performs 2 actions in the same transaction before to go to the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result['messages'][-1].content","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': '2024 Super Bowl winner'}, 'id': 'call_HBUU1Lo9WSgKCPKYCAStSb7g'}\n#Back to the model! \n\n(needs to come back with the answer of the first action)\n\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'Kansas City Chiefs headquarters location'}, 'id': 'call_qMwT4gLkDcvIlJmXDlW4jBll'}\n#Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'GDP of Missouri 2023'}, 'id': 'call_9lsnrIuDSFpdbEGMnY0VXaAN'}\n#Back to the model!\n\nFirst perform an action and use a tool for the first iteration\nThan go back to the model because it needs to know the answer before the next action\nAfter the answer it can perform the next action that is to call 2 times the same tool","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}